<!DOCTYPE HTML>
<!--
	Publications - Grégoire Dupont de Dinechin's portfolio website
-->
<html>
	<head>
		<title>Publications - Grégoire Dupont de Dinechin's portfolio website</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
	</head>
	<body class="is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<div id="main">
						<div class="inner">

							<!-- Header -->
								<header id="header">
									<a href="index.html" class="logo"><strong>Grégoire Dupont de Dinechin</strong>'s portfolio website</a>
									<ul class="icons">
										<li><a href="mailto:dinechingreg@gmail.com" class="icon solid fa-envelope"><span class="label">Email</span></a></li>
										<li><a href="https://www.linkedin.com/in/gregoire-dupont-de-dinechin/" class="icon solid fa-id-card"><span class="label">CV</span></a></li>
										<li><a href="https://www.linkedin.com/in/gregoire-dupont-de-dinechin/" class="icon brands fa-linkedin"><span class="label">LinkedIn</span></a></li>
										<li><a href="https://github.com/DinechinGreg" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
										<li><a href="https://scholar.google.fr/citations?user=dwhNbrkAAAAJ%26hl=en" class="icon ai ai-google-scholar"><span class="label">Google Scholar</span></a></li>
										<li><a href="https://www.researchgate.net/profile/Gregoire_Dupont_De_Dinechin" class="icon brands fa-researchgate"><span class="label">ResearchGate</span></a></li>
									</ul>
								</header>

							<!-- Content -->
								<section>
									<header class="main">
										<h1>Publications</h1>
									</header>
								
									<header class="major">
										<h2>Journal and conference papers</h2>
									</header>
									<div class="posts">
										<article id="journal-paper-impact-of-view-dependent">
											<span class="image"><img src="https://www.mdpi.com/applsci/applsci-11-06173/article_deploy/html/images/applsci-11-06173-g001-550.jpg" alt="" /></span>
											<header class="major">
												<h3>Journal paper</h3>
											</header>
											<p>Applied Sciences, Special Issue "New Frontiers in Virtual Reality: Methods, Devices and Applications", 2021</p>
											<h4>Impact of View-Dependent Image-Based Effects on Perception of Visual Realism and Presence in Virtual Reality Environments Created Using Multi-Camera Systems</h4>
											<p>Grégoire Dupont de Dinechin, Alexis Paljic, Jonathan Tanant</p>
											<nav class="simple_openers">
												<ul class="alt">
													<li></li>
													<li><span class="opener"><b>Abstract</b></span><ul><p>Several recent works have presented image-based methods for creating high-fidelity immersive virtual environments from photographs of real-world scenes. In this paper, we provide a user-centered evaluation of such methods by way of a user study investigating their impact on viewers' perception of visual realism and sense of presence. In particular, we focus on two specific elements commonly introduced by image-based approaches. First, we investigate the extent to which using dedicated image-based rendering algorithms to render the scene with view-dependent effects (such as specular highlights) causes users to perceive it as being more realistic. Second, we study whether making the scene fade out beyond a fixed volume in 3D space significantly reduces participants' feeling of being there, examining different sizes for this viewing volume. To provide details on the virtual environment used in the study, we also describe how we recreated a museum gallery for room-scale virtual reality using a custom-built multi-camera rig. The results of our study show that using image-based rendering to render view-dependent effects can effectively enhance the perception of visual realism and elicit a stronger sense of presence, even when it implies constraining the viewing volume to a small range of motion.</p></ul></li>
													<li><span class="opener"><b>BibTex</b></span><ul><p>Please cite the paper as:</p>
<pre><code>@article{deDinechin2021Impact,
	title = {Impact of View-Dependent Image-Based Effects on Perception of Visual Realism and Presence in Virtual Reality Environments Created Using Multi-Camera Systems},
	author = {de Dinechin, Gr{\'e}goire Dupont and Paljic, Alexis and Tanant, Jonathan},
	journal = {Applied Sciences},
	volume = {11},
	year = {2021},
	number = {13},
	article-number = {6173},
	issn = {2076-3417},
	url = {https://www.mdpi.com/2076-3417/11/13/6173},
	doi = {10.3390/app11136173}
}</code></pre>
													</ul></li>
													<li><span class="opener"><b>PDF</b></span><ul><p>Open access and published version: <a href="https://www.mdpi.com/2076-3417/11/13/6173">DOI</a></p></ul></li>
													<li></li>
												</ul>
											</nav>
										</article>
										<article id="conference-paper-from-real-to-virtual">
											<span class="video"><iframe src="https://www.youtube.com/embed/HtJarul_32c" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></span>
											<header class="major">
												<h3>Conference paper</h3>
											</header>
											<p>6th Workshop on Everyday Virtual Reality (WEVR), 2020</p>
											<h4>From Real to Virtual: An Image-Based Rendering Toolkit to Help Bring the World Around Us Into Virtual Reality</h4>
											<p>Grégoire Dupont de Dinechin, Alexis Paljic</p>
											<nav class="simple_openers">
												<ul class="alt">
													<li></li>
													<li><span class="opener"><b>Abstract</b></span><ul><p>The release of consumer-grade head-mounted displays has helped bring virtual reality (VR) to our homes, cultural sites, and workplaces, increasingly making it a part of our everyday lives. In response, many content creators have expressed renewed interest in bringing the people, objects, and places of our daily lives into VR, helping push the boundaries of our ability to transform photographs of everyday real-world scenes into convincing VR assets. In this paper, we present an open-source solution we developed in the Unity game engine as a way to make this image-based approach to virtual reality simple and accessible to all, to encourage content creators of all kinds to capture and render the world around them in VR. We start by presenting the use cases of image-based virtual reality, from which we discuss the motivations that led us to work on our solution. We then provide details on the development of the toolkit, specifically discussing our implementation of several image-based rendering (IBR) methods. Finally, we present the results of a preliminary user study focused on interface usability and rendering quality, and discuss paths for future work.</p></ul></li>
													<li><span class="opener"><b>BibTex</b></span><ul><p>Please cite the paper as:</p>
<pre><code>@inproceedings{deDinechin2020From,
	title = {From Real to Virtual: An Image-Based Rendering Toolkit to Help Bring the World Around Us Into Virtual Reality},
	booktitle = {6th Workshop on Everyday Virtual Reality ({WEVR})},
	publisher = {{IEEE}},
	author = {Gr{\'e}goire Dupont de Dinechin and Alexis Paljic},
	month = mar,
	year = {2020}
}</code></pre>
													</ul></li>
													<li><span class="opener"><b>PDF</b></span><ul><p>Published version: <a href="http://doi.org/10.1109/VRW50115.2020.00076">DOI</a><br/>Open access: <a href="https://hal.archives-ouvertes.fr/hal-02492896">HAL page</a> || <a href="https://hal.archives-ouvertes.fr/hal-02492896/document">PDF file</a></p></ul></li>
													<li><span class="opener"><b>Conference</b></span><ul><p>The <a href="https://wevr.adalsimeone.me/program2020">6th Workshop on Everyday Virtual Reality</a> took place on 22 March 2020, as a virtual event (due to the pandemic-related lockdown situation).<br/>The workshop was co-located with the <a href="http://ieeevr.org/2020">IEEE VR 2020</a> conference. It focused on "the investigation of well-known VR/AR/MR (XR) research themes in everyday contexts and scenarios other than research laboratories and specialist environments".</p></ul></li>
													<li></li>
												</ul>
											</nav>
										</article>
										<article id="conference-paper-virtual-agents-from-360">
											<span class="video"><iframe src="https://youtube.com/embed/Ve2Zzo0q-ck" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></span>
											<header class="major">
												<h3>Conference paper</h3>
											</header>
											<p>International Conference on Computer Animation and Social Agents (CASA), 2019</p>
											<h4>Virtual Agents from 360° Video for Interactive Virtual Reality</h4>
											<p>Grégoire Dupont de Dinechin, Alexis Paljic</p>
											<nav class="simple_openers">
												<ul class="alt">
													<li></li>
													<li><span class="opener"><b>Abstract</b></span><ul><p>Creating lifelike virtual humans for interactive virtual reality is a difficult task. Most current solutions rely either on crafting synthetic character models and animations, or on capturing real people with complex camera setups. As an alternative, we propose leveraging efficient learning-based models for human mesh estimation, and applying them to the popular form of immersive content that is 360° video. We demonstrate an implementation of this approach using available pre-trained models, and present user study results that show that the virtual agents generated with this method can be made more compelling by the use of idle animations and reactive verbal and gaze behavior.</p></ul></li>
													<li><span class="opener"><b>BibTex</b></span><ul><p>Please cite the paper as:</p>
<pre><code>@inproceedings{deDinechin2019Virtual,
	location = {Paris, France},
	series = {{CASA} '19},
	title = {Virtual Agents from 360° Video for Interactive Virtual Reality},
	isbn = {978-1-4503-7159-9},
	url = {https://doi.org/10.1145/3328756.3328775},
	doi = {10.1145/3328756.3328775},
	booktitle = {Proceedings of the 32nd International Conference on Computer Animation and Social Agents},
	publisher = {{ACM}},
	author = {Gr{\'e}goire Dupont de Dinechin and Alexis Paljic},
	month = jul,
	year = {2019},
	pages = {75--78}
}</code></pre>
													</ul></li>
													<li><span class="opener"><b>PDF</b></span><ul><p>Published version: <a href="http://doi.org/10.1145/3328756.3328775">DOI</a><br/>Open access: <a href="https://hal.archives-ouvertes.fr/hal-02155594">HAL page</a> || <a href="https://hal.archives-ouvertes.fr/hal-02155594/document">PDF file</a></p></ul></li>
													<li><span class="opener"><b>Conference</b></span><ul><p><a href="https://casa2019.sciencesconf.org/">CASA 2019</a> (32nd International Conference on Computer Animation and Social Agents) took place on 1-3 July in Paris, France.<br/>Organised in cooperation with ACM-SIGGRAPH and jointly with the 2019 International Conference on Intelligent Virtual Agents (IVA 2019), the conference presented works on "computer animation, embodied agents, social agents, virtual and augmented reality, and visualization".</p></ul></li>
													<li></li>
												</ul>
											</nav>
										</article>
										<article id="conference-paper-cinematic-virtual-reality">
											<span class="video"><iframe src="https://youtube.com/embed/mYhh_DZddg8" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe></span>
											<header class="major">
												<h3>Conference paper</h3>
											</header>
											<p>3rd Digital Heritage International Congress (DigitalHERITAGE) held jointly with the 24th International Conference on Virtual Systems & Multimedia (VSMM), 2018</p>
											<h4>Cinematic Virtual Reality With Motion Parallax From a Single Monoscopic Omnidirectional Image</h4>
											<p>Grégoire Dupont de Dinechin, Alexis Paljic</p>
											<nav class="simple_openers">
												<ul class="alt">
													<li></li>
													<li><span class="opener"><b>Abstract</b></span><ul><p>Complementary advances in the fields of virtual reality (VR) and reality capture have led to a growing demand for VR experiences that enable users to convincingly move around in an environment created from a real-world scene. Most methods address this issue by first acquiring a large number of image samples from different viewpoints. However, this is often costly in both time and hardware requirements, and is incompatible with the growing selection of existing, casually-acquired 360-degree images available online. In this paper, we present a novel solution for cinematic VR with motion parallax that instead only uses a single monoscopic omnidirectional image as input. We provide new insights on how to convert such an image into a scene mesh, and discuss potential uses of this representation. We notably propose using a VR interface to manually generate a 360-degree depth map, visualized as a 3D mesh and modified by the operator in real-time. We applied our method to different real-world scenes, and conducted a user study comparing meshes created from depth maps of different levels of accuracy. The results show that our method enables perceptually comfortable VR viewing when users move around in the scene.</p></ul></li>
													<li><span class="opener"><b>BibTex</b></span><ul><p>Please cite the paper as:</p>
<pre><code>@inproceedings{deDinechin2018Cinematic,
	title = {Cinematic Virtual Reality With Motion Parallax From a Single Monoscopic Omnidirectional Image},
	url = {https://doi.org/10.1109/digitalheritage.2018.8810116},
	doi = {10.1109/digitalheritage.2018.8810116},
	booktitle = {2018 3rd Digital Heritage International Congress ({DigitalHERITAGE}) held jointly with 2018 24th International Conference on Virtual Systems \& Multimedia ({VSMM} 2018)},
	author = {Gr{\'e}goire Dupont de Dinechin and Alexis Paljic},
	publisher = {{IEEE}},
	month = oct,
	year = {2018},
	pages = {1--8}
}</code></pre>
													</ul></li>
													<li><span class="opener"><b>PDF</b></span><ul><p>Published version: <a href="http://doi.org/10.1109/DigitalHeritage.2018.8810116">DOI</a><br/>Open access: <a href="https://hal-mines-paristech.archives-ouvertes.fr/hal-01915197">HAL page</a> || <a href="https://hal-mines-paristech.archives-ouvertes.fr/hal-01915197/document">PDF file</a></p></ul></li>
													<li><span class="opener"><b>Conference</b></span><ul><p><a href="https://doi.org/10.1109/DigitalHeritage44163.2018">Digital Heritage 2018</a> (New Realities - Authenticity and Automation in the Digital Age, 3rd International Congress and Expo) took place on 26-30 October 2018 in San Francisco, USA.<br/>Focused on "digital technology for documenting, conserving and sharing heritage", it included the 24th International Conference on Virtual Systems and MultiMedia (VSMM 2018) and the 25th Conference of the Pacific Neighborhood Consortium (PNC 2018).<br/>Research tracks included works on reality capture (digitization, scanning, remote sensing, …), reality computing (databases and repositories, GIS, CAD, …) and reality creation (VR, AR, MR, …).</p></ul></li>
													<li><span class="opener"><b>Datasets</b></span><ul><p>The four color and depth 360° image pairs used for the user study (Garden, Bookshelves, Snow, Museum) can be downloaded by clicking <a href="https://drive.google.com/drive/folders/1IzXdfWxHkK_o4ftTqcDA1b0uzQck7RBz?usp=sharing">here</a>.</p></ul></li>
													<li></li>
												</ul>
											</nav>
										</article>
										<article>
										</article>
										<article>
										</article>
									</div>
									<br/>
									<br/>
									<br/>
									
									<header class="major">
										<h2>Posters, videos, and research demonstrations</h2>
									</header>
									<div class="posts">
										<article id="poster-presenting-colibri-vr">
											<header class="major">
												<h3>Poster</h3>
											</header>
											<p>IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR), 2020</p>
											<h4>Presenting COLIBRI VR, an Open-Source Toolkit to Render Real-World Scenes in Virtual Reality</h4>
											<p>Grégoire Dupont de Dinechin, Alexis Paljic</p>
											<nav class="simple_openers">
												<ul class="alt">
													<li></li>
													<li><span class="opener"><b>Abstract</b></span><ul><p>From image-based virtual tours of apartments to digital museum exhibits, transforming photographs of real-world scenes into visually faithful virtual environments has many applications. In this paper, we present our development of a toolkit that places recent advances in the field of image-based rendering (IBR) into the hands of virtual reality (VR) researchers and content creators. We map out how these advances can improve the way we usually render virtual scenes from photographs. We then provide insight into the toolkit’s design as a package for the Unity game engine and share details on core elements of our implementation.</p></ul></li>
													<li><span class="opener"><b>BibTex</b></span><ul><p>Please cite the 2-page poster abstract as:</p>
<pre><code>@inproceedings{deDinechin2020Presenting,
	title = {Presenting {COLIBRI VR}, an Open-Source Toolkit to Render Real-World Scenes in Virtual Reality},
	booktitle = {2020 {IEEE} Conference on Virtual Reality and {3D} User Interfaces ({VR})},
	publisher = {{IEEE}},
	author = {Gr{\'e}goire Dupont de Dinechin and Alexis Paljic},
	month = mar,
	year = {2020}
}</code></pre>
													</ul></li>
													<li><span class="opener"><b>PDF</b></span><ul><p>Published version: <a href="http://doi.org/10.1109/VRW50115.2020.00251">DOI</a><br/>Open access: <a href="https://hal.archives-ouvertes.fr/hal-02492722">HAL page</a> || <a href="https://hal.archives-ouvertes.fr/hal-02492722/document">PDF file (2-page abstract)</a></p></ul></li>
													<li><span class="opener"><b>Conference</b></span><ul><p><a href="http://ieeevr.org/2020/">IEEE VR 2020</a> (the 27th IEEE Conference on Virtual Reality and 3D User Interfaces) took place on 22-26 March 2020, as a virtual event (due to the lockdown situation).<br/>IEEE VR is "the premier international event for the presentation of research results in the broad area of virtual reality (VR)".</p></ul></li>
													<li></li>
												</ul>
											</nav>
										</article>
										<article id="research-demonstration-demonstrating-colibri-vr">
											<header class="major">
												<h3>Research demonstration</h3>
											</header>
											<p>IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR), 2020</p>
											<h4>Demonstrating COLIBRI VR, an Open-Source Toolkit to Render Real-World Scenes in Virtual Reality</h4>
											<p>Grégoire Dupont de Dinechin, Alexis Paljic</p>
											<nav class="simple_openers">
												<ul class="alt">
													<li></li>
													<li><span class="opener"><b>Abstract</b></span><ul><p>This demonstration showcases an open-source toolkit we developed in the Unity game engine to enable authors to render real-world photographs in virtual reality (VR) with motion parallax and view-dependent highlights. First, we illustrate the toolset's capabilities by using it to display interactive, photorealistic renderings of a museum's mineral collection. Then, we invite audience members to be rendered in VR using our toolkit, thus providing a live, behind-the-scenes look at the process.</p></ul></li>
													<li><span class="opener"><b>BibTex</b></span><ul><p>Please cite the 2-page abstract of the research demonstration as:</p>
<pre><code>@inproceedings{deDinechin2020Demonstrating,
	title = {Demonstrating {COLIBRI VR}, an Open-Source Toolkit to Render Real-World Scenes in Virtual Reality},
	booktitle = {2020 {IEEE} Conference on Virtual Reality and {3D} User Interfaces ({VR})},
	publisher = {{IEEE}},
	author = {Gr{\'e}goire Dupont de Dinechin and Alexis Paljic},
	month = mar,
	year = {2020}
}</code></pre>
													</ul></li>
													<li><span class="opener"><b>PDF</b></span><ul><p>Published version: <a href="http://doi.org/10.1109/VRW50115.2020.00273">DOI</a><br/>Open access: <a href="https://hal.archives-ouvertes.fr/hal-02492733">HAL page</a> || <a href="https://hal.archives-ouvertes.fr/hal-02492733/document">PDF file (2-page abstract)</a></p></ul></li>
													<li><span class="opener"><b>Conference</b></span><ul><p><a href="http://ieeevr.org/2020/">IEEE VR 2020</a> (the 27th IEEE Conference on Virtual Reality and 3D User Interfaces) took place on 22-26 March 2020, as a virtual event (due to the lockdown situation).<br/>IEEE VR is "the premier international event for the presentation of research results in the broad area of virtual reality (VR)".</p></ul></li>
													<li></li>
												</ul>
											</nav>
										</article>
										<article id="video-illustrating-colibri-vr">
											<header class="major">
												<h3>Video submission</h3>
											</header>
											<p>IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR), 2020</p>
											<h4>Illustrating COLIBRI VR, an Open-Source Toolkit to Render Real-World Scenes in Virtual Reality</h4>
											<p>Grégoire Dupont de Dinechin, Alexis Paljic</p>
											<nav class="simple_openers">
												<ul class="alt">
													<li></li>
													<li><span class="opener"><b>Abstract</b></span><ul><p>This video submission illustrates the Core Open Lab on Image-Based Rendering Innovation for Virtual Reality (COLIBRI VR), an open-source toolkit we developed to help authors render photographs of real-world people, objects, and places as responsive 3D assets in VR. We integrated COLIBRI VR as a package for the Unity game engine: in this way, the toolset's methods can easily be accessed from a convenient graphical user interface, and be used in conjunction with the game engine's built-in tools to quickly build interactive virtual reality experiences. Our primary goal is to help users render real-world photographs in VR in a way that provides view-dependent rendering effects and compelling motion parallax. For instance, COLIBRI VR can be used to render captured specular highlights, such as the bright reflections on the facets of a mineral. It also enables providing motion parallax from estimated geometry, e.g. from a depth map associated to a 360° image. We achieve this by implementing efficient image-based rendering methods, which we optimize to run at high framerates for VR. We make the toolkit openly available online, so that it might be used to more easily learn about and apply image-based rendering in the context of virtual reality content creation.</p></ul></li>
													<li><span class="opener"><b>BibTex</b></span><ul><p>Please cite the 1-page abstract of the video submission as:</p>
<pre><code>@inproceedings{deDinechin2020Illustrating,
	title = {Illustrating {COLIBRI VR}, an Open-Source Toolkit to Render Real-World Scenes in Virtual Reality},
	booktitle = {2020 {IEEE} Conference on Virtual Reality and {3D} User Interfaces ({VR})},
	publisher = {{IEEE}},
	author = {Gr{\'e}goire Dupont de Dinechin and Alexis Paljic},
	month = mar,
	year = {2020}
}</code></pre>
													</ul></li>
													<li><span class="opener"><b>PDF</b></span><ul><p>Published version: <a href="http://doi.org/10.1109/VRW50115.2020.00280">DOI</a><br/>Open access: <a href="https://hal.archives-ouvertes.fr/hal-02492741">HAL page</a> || <a href="https://hal.archives-ouvertes.fr/hal-02492741/file/deDinechinPaljic2020Illustrating.pdf">PDF file (1-page abstract)</a></p></ul></li>
													<li><span class="opener"><b>Conference</b></span><ul><p><a href="http://ieeevr.org/2020/">IEEE VR 2020</a> (the 27th IEEE Conference on Virtual Reality and 3D User Interfaces) took place on 22-26 March 2020, as a virtual event (due to the lockdown situation).<br/>IEEE VR is "the premier international event for the presentation of research results in the broad area of virtual reality (VR)".</p></ul></li>
													<li></li>
												</ul>
											</nav>
										</article>
										<article id="poster-automatic-generation-of-interactive">
											<header class="major">
												<h3>Poster</h3>
											</header>
											<p>IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR), 2019</p>
											<h4>Automatic Generation of Interactive 3D Characters and Scenes for Virtual Reality from a Single-Viewpoint 360-Degree Video</h4>
											<p>Grégoire Dupont de Dinechin, Alexis Paljic</p>
											<nav class="simple_openers">
												<ul class="alt">
													<li></li>
													<li><span class="opener"><b>Abstract</b></span><ul><p>This work addresses the problem of using real-world data captured from a single viewpoint by a low-cost 360-degree camera to create an immersive and interactive virtual reality scene. We combine different existing state-of-the-art data enhancement methods based on pre-trained deep learning models to quickly and automatically obtain 3D scenes with animated character models from a 360-degree video. We provide details on our implementation and insight on how to adapt existing methods to 360-degree inputs. We also present the results of a user study assessing the extent to which virtual agents generated by this process are perceived as present and engaging.</p></ul></li>
													<li><span class="opener"><b>BibTex</b></span><ul><p>Please cite the 2-page poster abstract as:</p>
<pre><code>@inproceedings{deDinechin2019Automatic,
	title = {Automatic Generation of Interactive {3D} Characters and Scenes for Virtual Reality from a Single-Viewpoint 360-Degree Video},
	url = {https://doi.org/10.1109/vr.2019.8797969},
	doi = {10.1109/vr.2019.8797969},
	booktitle = {2019 {IEEE} Conference on Virtual Reality and {3D} User Interfaces ({VR})},
	author = {Gr{\'e}goire Dupont de Dinechin and Alexis Paljic},
	publisher = {{IEEE}},
	month = mar,
	year = {2019},
	pages = {908--909}
}</code></pre>
													</ul></li>
													<li><span class="opener"><b>PDF</b></span><ul><p>Published version: <a href="http://doi.org/10.1109/VR.2019.8797969">DOI</a><br/>Open access: <a href="https://hal.archives-ouvertes.fr/hal-02114141v2">HAL page</a> || <a href="https://hal.archives-ouvertes.fr/hal-02114141v2/document">PDF file (2-page abstract)</a> || <a href="https://hal.archives-ouvertes.fr/hal-02114141v1/document">PDF file (poster)</a></p></ul></li>
													<li><span class="opener"><b>Conference</b></span><ul><p><a href="http://ieeevr.org/2019/">IEEE VR 2019</a> (26th IEEE Conference on Virtual Reality and 3D User Interfaces) took place on 23-27 March in Osaka, Japan.<br/>Focused on "all areas related to virtual reality (VR), including augmented reality (AR), mixed reality (MR),and 3D user interfaces (3DUIs)", the conference was an opportunity to present works on technologies and applications (computer graphics, immersive 360° video, modelling and simulation, …), multi-sensory experiences (virtual humans, haptics, perception and cognition, …) and interaction (collaborative interaction, locomotion and navigation, multimodal interaction, …).</p></ul></li>
													<li></li>
												</ul>
											</nav>
										</article>
										<article>
										</article>
										<article>
										</article>
									</div>

									<header class="major">
										<h2>News</h2>
									</header>
									<header class="major">
										<h3>2021</h3>
									</header>
									<ul class="alt">
										<li></li>
										<li><i>28 June 2021</i><br/>Our paper submission to <a href="https://www.mdpi.com/journal/applsci/special_issues/New_Frontiers_in_Virtual_Reality_MDA">Applied Sciences, Special Issue "New Frontiers in Virtual Reality: Methods, Devices and Applications"</a> was accepted! More information on the paper can be found on <a href="publications.html#journal-paper-impact-of-view-dependent">this page</a>.</li>
										<li></li>
									</ul>
									<header class="major">
										<h3>2020</h3>
									</header>
									<ul class="alt">
										<li></li>
										<li><i>18 December 2020</i><br/>I defended my PhD thesis, entitled "Towards comfortable virtual reality viewing of virtual environments created from photographs of the real world". The members of my jury were: Anatole Lécuyer, Anthony Steed, Selma Rizvic, Diego Gutierrez, Jean-Philippe Farrugia, and Alexis Paljic.<br/>Details on the manuscript can be found at <a href="https://theses.fr/2020UPSLM049">this link</a>, on the official website for French PhD theses. Here is also a direct link to the manuscript <a href="https://pastel.archives-ouvertes.fr/tel-03142417/document">PDF</a>.</li>
										<li><i>22-26 March 2020</i><br/>Alexis Paljic and I virtually attended <a href="http://ieeevr.org/2020/">IEEE VR 2020</a>, held this year as an online event via the Mozilla Hubs VR platform. I presented the COLIBRI VR open-source project, most notably via the workshop paper "From Real to Virtual: An Image-Based Rendering Toolkit to Help Bring the World Around Us Into Virtual Reality".</li>
										<li><i>12 February 2020</i><br/>Our paper submission to the <a href="https://wevr.adalsimeone.me/program2020">6th Workshop on Everyday Virtual Reality (WEVR 2020)</a> was accepted! More information on the paper can be found on <a href="publications.html#conference-paper-from-real-to-virtual">this page</a>.<br/>Our poster submission to the <a href="http://ieeevr.org/2020/">2020 IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2020)</a> was accepted! More information on the poster can be found on <a href="publications.html#poster-presenting-colibri-vr">this page</a>.</li>
										<li><i>28 January 2020</i><br/>Our video submission to the <a href="http://ieeevr.org/2020/">2020 IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2020)</a> was accepted! More information on the video can be found on <a href="publications.html#video-illustrating-colibri-vr">this page</a>.</li>
										<li><i>13 January 2020</i><br/>Our research demonstration submission to the <a href="http://ieeevr.org/2020/">2020 IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2020)</a> was accepted! More information on the research demonstration can be found on <a href="publications.html#research-demonstration-demonstrating-colibri-vr">this page</a>.</li>
										<li></li>
									</ul>
									<header class="major">
										<h3>2019</h3>
									</header>
									<ul class="alt">
										<li></li>
										<li><i>02 July 2019</i><br/>I attended <a href="https://casa2019.sciencesconf.org/">CASA 2019</a> in Paris (France). I presented "Virtual Agents from 360° Video for Interactive Virtual Reality".</li>
										<li><i>25 April 2019</i><br/>Our submission to the <a href="https://casa2019.sciencesconf.org/">2019 International Conference on Computer Animation and Social Agents (CASA 2019)</a> was accepted as a short paper! More information on the paper can be found on <a href="publications.html#conference-paper-virtual-agents-from-360">this page</a>.</li>
										<li><i>25-27 March 2019</i><br/>Alexis Paljic and I attended <a href="http://ieeevr.org/2019/">IEEE VR 2019</a> in Osaka (Japan). I presented "Automatic Generation of Interactive 3D Characters and Scenes for Virtual Reality from a Single-Viewpoint 360-Degree Video".</li>
										<li><i>05 February 2019</i><br/>Our submission to the <a href="http://ieeevr.org/2019/">2019 IEEE Conference on Virtual Reality and 3D User Interfaces (IEEE VR 2019)</a> was accepted as a poster! More information on the poster can be found on <a href="publications.html#poster-automatic-generation-of-interactive">this page</a>.</li>
										<li></li>
									</ul>
									<header class="major">
										<h3>2018</h3>
									</header>
									<ul class="alt">
										<li></li>
										<li><i>26-30 October 2018</i><br/>I attended <a href="https://doi.org/10.1109/DigitalHeritage44163.2018">Digital Heritage 2018</a> in San Francisco (USA) to present "Cinematic Virtual Reality With Motion Parallax from a Single Monoscopic Omnidirectional Image".</li>
										<li><i>08 August 2018</i><br/>Our submission to <a href="https://doi.org/10.1109/DigitalHeritage44163.2018">Digital Heritage 2018</a> was accepted as a full paper! More information on the paper can be found on  <a href="publications.html#conference-paper-cinematic-virtual-reality">this page</a>.</li>
										<li></li>
									</ul>
									<header class="major">
										<h3>2017</h3>
									</header>
									<ul class="alt">
										<li></li>
										<li><i>01 October 2017</i><br/>I started my PhD at the Virtual and Augmented Reality team of <a href="http://www.mines-paristech.eu/">MINES ParisTech</a>'s <a href="http://caor-mines-paristech.fr/en/home/">Centre for Robotics</a>, under the supervision of Alexis Paljic.</li>
										<li></li>
									</ul>

								</section>

						</div>
					</div>

				<!-- Sidebar -->
					<div id="sidebar">
						<div class="inner">

							<!-- Contact -->
							<section>
								<header class="major">
									<h2>Contact</h2>
								</header>
								<ul class="contact">
									<li class="icon solid fa-envelope"><a href="mailto:dinechingreg@gmail.com">dinechingreg@gmail.com</a></li>
								</ul>
							</section>

							<!-- Menu -->
								<nav id="menu">
									<header class="major">
										<h2>Menu</h2>
									</header>
									<ul>
										<li><a href="index.html">Homepage</a></li>
										<li><a href="projects.html">Projects</a></li>
										<li><a href="publications.html">Publications</a></li>
									</ul>
								</nav>

							<!-- Footer -->
								<footer id="footer">
									<p class="copyright">Made by Grégoire Dupont de Dinechin, using the Editorial template from <a href="https://html5up.net">HTML5 UP</a>.</p>
								</footer>

						</div>
					</div>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>